{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import GRU\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.layers import Input, LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path and data types\n",
    "file_path = \"data/preprocessed_stock_data.csv\"\n",
    "dtype_dict = {'Ticker': 'category', 'Close': 'float32', 'Volume': 'float32', 'Price_Change': 'float32', \n",
    "              'Daily_Return': 'float32', 'Volatility': 'float32', 'MA_5': 'float32', 'MA_10': 'float32'}\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    na_values=['null'],\n",
    "    index_col='Date',\n",
    "    parse_dates=True,\n",
    "    infer_datetime_format=True,\n",
    "    dtype=dtype_dict\n",
    ")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"First few rows of dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"Dataset information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for the shape of the dataframe and missing values\n",
    "print(\"Dataframe Shape: \", df.shape)\n",
    "print(\"Null Values Present: \", df.isnull().values.any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by Ticker and plot the 'Close' prices\n",
    "grouped = df.groupby('Ticker')\n",
    "\n",
    "# Plot 'Close' values for each 'Ticker'\n",
    "for ticker, data in grouped:\n",
    "    plt.figure(figsize=(10, 5))  # Set figure size for each plot\n",
    "    data['Close'].plot(title=f\"Closing prices for {ticker}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Close\")\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['Volume', 'Price_Change', 'Daily_Return', 'Volatility','MA_5', 'MA_10']\n",
    "scaler = MinMaxScaler()\n",
    "# Check for NaN or infinite values\n",
    "print(\"Checking for NaN or infinite values in the data...\")\n",
    "print(df[features].isnull().sum())  # Check for NaN values\n",
    "print((df[features] == np.inf).sum())  # Check for infinity values\n",
    "print((df[features] == -np.inf).sum())  # Check for negative infinity values\n",
    "\n",
    "# Replace or drop NaN or infinite values\n",
    "df[features] = df[features].replace([np.inf, -np.inf], np.nan)  # Replace infinities with NaN\n",
    "df = df.dropna(subset=features)  # Drop rows with NaN values\n",
    "\n",
    "# Now scale the data\n",
    "scaled_data_list = []  # Use a list to store data for faster concatenation\n",
    "\n",
    "for ticker, group in df.groupby('Ticker'):\n",
    "    # Scale features within each ticker group\n",
    "    scaled_features = scaler.fit_transform(group[features])\n",
    "    \n",
    "    # Create a DataFrame for the scaled features\n",
    "    scaled_group = pd.DataFrame(\n",
    "        data=scaled_features,\n",
    "        columns=features,\n",
    "        index=group.index\n",
    "    )\n",
    "    \n",
    "    # Add the 'Ticker' column back to the scaled group\n",
    "    scaled_group['Ticker'] = ticker\n",
    "    \n",
    "    # Append the scaled group to the list\n",
    "    scaled_data_list.append(scaled_group)\n",
    "\n",
    "# Concatenate all scaled groups at once\n",
    "scaled_data = pd.concat(scaled_data_list)\n",
    "\n",
    "# Display the first few rows of the scaled data\n",
    "print(\"Scaled data (first few rows):\")\n",
    "print(scaled_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Ticker' and 'Date' to maintain time order\n",
    "df = df.sort_values(by=['Ticker', 'Date'])\n",
    "# Initialize TimeSeriesSplit with 10 splits\n",
    "timesplit = TimeSeriesSplit(n_splits=10)\n",
    "output_var = ['Price_Change']\n",
    "# Initialize lists to store the train and test data for each split\n",
    "X_train_list, X_test_list, y_train_list, y_test_list = [], [], [], []\n",
    "\n",
    "# Perform time series split\n",
    "for train_index, test_index in timesplit.split(df):\n",
    "    # Use loc instead of iloc for clarity with index handling\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "    \n",
    "    # Extract features and target variables for train and test\n",
    "    X_train_list.append(train_data[features].values)  # Convert to NumPy for efficiency\n",
    "    X_test_list.append(test_data[features].values)\n",
    "    y_train_list.append(train_data[output_var].values)\n",
    "    y_test_list.append(test_data[output_var].values)\n",
    "\n",
    "# Concatenate all splits into single arrays\n",
    "X_train_all = np.vstack(X_train_list)  # Stack arrays vertically\n",
    "X_test_all = np.vstack(X_test_list)\n",
    "y_train_all = np.concatenate(y_train_list)\n",
    "y_test_all = np.concatenate(y_test_list)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"X_train_all shape:\", X_train_all.shape)\n",
    "print(\"X_test_all shape:\", X_test_all.shape)\n",
    "print(\"y_train_all shape:\", y_train_all.shape)\n",
    "print(\"y_test_all shape:\", y_test_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for LSTM\n",
    "X_train_all = X_train_all.reshape(X_train_all.shape[0], 1, X_train_all.shape[1])  # Add time step dimension (1)\n",
    "X_test_all = X_test_all.reshape(X_test_all.shape[0], 1, X_test_all.shape[1])  # Add time step dimension (1)\n",
    "\n",
    "# Now define the LSTM model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, X_train_all.shape[2]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Display the model summary\n",
    "plot_model(lstm, show_shapes=True, show_layer_names=True)\n",
    "lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add callbacks for optimization\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss', \n",
    "    patience=5,  # Stop training if loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss', \n",
    "    factor=0.5, \n",
    "    patience=3,  # Reduce learning rate if loss doesn't improve for 3 epochs\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Train the model with fewer epochs and callbacks\n",
    "history = lstm.fit(\n",
    "    X_train_all, \n",
    "    y_train_all, \n",
    "    epochs=50,  # Reduce epochs\n",
    "    batch_size=16,  # Larger batch size for faster training\n",
    "    verbose=1, \n",
    "    shuffle=False, \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Prediction\n",
    "y_pred_all = lstm.predict(X_test_all, batch_size=8, verbose=1)\n",
    "\n",
    "# Flatten the predicted values and actual values for comparison\n",
    "y_pred_all = y_pred_all.flatten()  # Flatten predictions\n",
    "y_test_all = y_test_all.flatten()  # Flatten true values (if necessary)\n",
    "\n",
    "# Print the first few predictions and corresponding actual values for verification\n",
    "print(\"First 10 predictions:\", y_pred_all[:10])\n",
    "print(\"First 10 actual values:\", y_test_all[:10])\n",
    "\n",
    "# Calculate performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test_all, y_pred_all)\n",
    "mae = mean_absolute_error(y_test_all, y_pred_all)\n",
    "r2 = r2_score(y_test_all, y_pred_all)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2 Score):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU Model Intergration\n",
    "#Reshape the date for GRU\n",
    "X_train_all_gru = X_train_all.reshape(X_train_all.shape[0], X_train_all.shape[1], X_train_all.shape[2])\n",
    "\n",
    "#Define the Gru model\n",
    "gru = Sequential()\n",
    "gru.add(GRU(32, input_shape=(X_train_all_gru.shape[1], X_train_all_gru.shape[2]), activation='relu', return_sequences=False))\n",
    "gru.add(Dense(1))\n",
    "\n",
    "#Compile the gru model\n",
    "gru.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gru prediction\n",
    "y_pred_gru = gru.predict(X_test_all.reshape(X_test_all.shape[0], X_test_all.shape[1], X_test_all.shape[2]))\n",
    "y_pred_gru = y_pred_gru.flatten()\n",
    "\n",
    "#Calculate GRU performance metrics\n",
    "mse_gru = mean_squared_error(y_test_all, y_pred_gru)\n",
    "mae_gru = mean_absolute_error(y_test_all, y_pred_gru)\n",
    "r2_gru = r2_score(y_test_all, y_pred_gru)\n",
    "\n",
    "print(\"GRU Mean Squared Error:\", mse_gru)\n",
    "print(\"GRU Mean Absolute Error:\", mae_gru)\n",
    "print(\"GRU R-squared:\", r2_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15490/21537\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0092"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m transformer \u001b[38;5;241m=\u001b[39m transformer_model((X_train_all\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train_all\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#Train the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m history_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#calculate\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y_pred_transformer \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mpredict(X_test_all)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Khai Cao\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Transformer model integration\n",
    "#define the transformer model\n",
    "def transformer_model (input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = MultiHeadAttention(num_heads=2, key_dim=16)(inputs, inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "transformer = transformer_model((X_train_all.shape[1], X_train_all.shape[2]))\n",
    "\n",
    "#Train the model\n",
    "history_transformer = transformer.fit(X_train_all,y_train_all, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "#calculate\n",
    "y_pred_transformer = transformer.predict(X_test_all)\n",
    "\n",
    "#calculate performance\n",
    "mse_xgb = mean_squared_error(y_test_all,y_pred_transformer)\n",
    "mae_xgb = mean_absolute_error(y_test_all,y_pred_transformer)\n",
    "r2_xgb = r2(y_test_all,y_pred_transformer)\n",
    "\n",
    "print(\"XGBoost Mean Squared Error:\", mse_xgb)\n",
    "print(\"XGBoost Mean Absolute Error:\", mae_xgb)\n",
    "print(\"XGBoost R-Squared:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Model Intergration\n",
    "#Prepare dât for model\n",
    "X_train_xgb = X_train_all.reshape(X_train_all.shape[0],X_train_all.shape[1], X_train_all.shape[2])\n",
    "X_test_xgb = X_test_all.reshape(X_test_all.shape[0],X_test_all.shape[2])\n",
    "\n",
    "#create dmtraix for model\n",
    "dtrain = xgb.DMatrix(X_train_xgb,label=y_train_all)\n",
    "dtest = xgb.DMatrix(X_test_xgb,label=X_test_all)\n",
    "\n",
    "#set parameters for model\n",
    "params={\n",
    "    'objective':'reg:squarederror',\n",
    "    'max_depth':6,\n",
    "    'learning_rate':0.1,\n",
    "    'n_estimators':100,\n",
    "    'eval_metric':'rmse'\n",
    "}\n",
    "\n",
    "#train\n",
    "xgb_model=xgb.train(params,dtrain)\n",
    "\n",
    "#prediction\n",
    "y_pred_xgb = xgb_model.predict(dtest)\n",
    "\n",
    "#calculate performance\n",
    "mse_xgb = mean_squared_error(y_test_all,y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test_all,y_pred_xgb)\n",
    "r2_xgb = r2(y_test_all,y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Mean Squared Error:\", mse_xgb)\n",
    "print(\"XGBoost Mean Absolute Error:\", mae_xgb)\n",
    "print(\"XGBoost R-Squared:\", r2_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of indices for each ticker\n",
    "ticker_indices = {}\n",
    "start_index = 0\n",
    "for ticker, group in grouped:\n",
    "    group_len = len(group)\n",
    "    ticker_indices[ticker] = range(start_index, start_index + group_len)\n",
    "    start_index += group_len\n",
    "\n",
    "# LSTM Prediction\n",
    "y_pred_all = lstm.predict(X_test_all, batch_size=8, verbose=0).flatten()  # Flatten predictions\n",
    "\n",
    "# Plot Predicted vs True for each ticker\n",
    "for ticker, indices in ticker_indices.items():\n",
    "    # Extract valid indices (ensure they are within the test set range)\n",
    "    valid_indices = [idx for idx in indices if idx < len(y_test_all)]\n",
    "\n",
    "    if not valid_indices:\n",
    "        print(f\"No valid indices for ticker {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Extract true and predicted values for the current ticker\n",
    "    y_test_ticker = y_test_all[valid_indices]\n",
    "    y_pred_ticker = y_pred_all[valid_indices]\n",
    "\n",
    "    # Plot the true and predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_ticker, label=f'True Value for {ticker}', color='blue', linestyle='--')\n",
    "    plt.plot(y_pred_ticker, label=f'LSTM Predicted Value for {ticker}', color='red', linestyle='-')\n",
    "    plt.title(f\"Prediction by LSTM for {ticker}\")\n",
    "    plt.xlabel('Time Scale')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ đồ thị GRU\n",
    "for ticker, indices in ticker_indices.items():\n",
    "    valid_indices = [idx for idx in indices if idx < len(y_test_all)]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(f\"No valid indices for ticker {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    y_test_ticker = y_test_all[valid_indices]\n",
    "    y_pred_ticker = y_pred_gru[valid_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_ticker, label=f'True Value for {ticker}', color='blue', linestyle='--')\n",
    "    plt.plot(y_pred_ticker, label=f'GRU Predicted Value for {ticker}', color='green', linestyle='-')\n",
    "    plt.title(f\"Prediction by GRU for {ticker}\")\n",
    "    plt.xlabel('Time Scale')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ đồ thị XGBoost\n",
    "for ticker, indices in ticker_indices.items():\n",
    "    valid_indices = [idx for idx in indices if idx < len(y_test_all)]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(f\"No valid indices for ticker {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    y_test_ticker = y_test_all[valid_indices]\n",
    "    y_pred_ticker = y_pred_xgb[valid_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_ticker, label=f'True Value for {ticker}', color='blue', linestyle='--')\n",
    "    plt.plot(y_pred_ticker, label=f'XGBoost Predicted Value for {ticker}', color='orange', linestyle='-')\n",
    "    plt.title(f\"Prediction by XGBoost for {ticker}\")\n",
    "    plt.xlabel('Time Scale')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ đồ thị Transformer\n",
    "for ticker, indices in ticker_indices.items():\n",
    "    valid_indices = [idx for idx in indices if idx < len(y_test_all)]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(f\"No valid indices for ticker {ticker}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    y_test_ticker = y_test_all[valid_indices]\n",
    "    y_pred_ticker = y_pred_transformer[valid_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test_ticker, label=f'True Value for {ticker}', color='blue', linestyle='--')\n",
    "    plt.plot(y_pred_ticker, label=f'Transformer Predicted Value for {ticker}', color='purple', linestyle='-')\n",
    "    plt.title(f\"Prediction by Transformer for {ticker}\")\n",
    "    plt.xlabel('Time Scale')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
